{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOp0j43ec9CBbZNKTb1FpaH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robgon-art/faster-whisper-test/blob/main/CTranslate2_Whisper_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ctranslate2 transformers"
      ],
      "metadata": {
        "id": "YHc4krqK2KnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/jordimas/whisper-cpp-error/raw/main/15GdH9-curt.mp3"
      ],
      "metadata": {
        "id": "O45oYOjs3oE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ct2-transformers-converter --model openai/whisper-tiny --output_dir whisper-tiny-ct2\n",
        "!ct2-transformers-converter --model openai/whisper-tiny --output_dir whisper-tiny-ct2-int8 --quantization int8"
      ],
      "metadata": {
        "id": "xl76h6XE2rsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ctranslate2\n",
        "import librosa\n",
        "import transformers\n",
        "import logging\n",
        "\n",
        "# Load and resample the audio file.\n",
        "audio, _ = librosa.load(\"/content/15GdH9-curt.mp3\", sr=16000, mono=True)\n",
        "\n",
        "# Compute the features of the first 30 seconds of audio.\n",
        "processor = transformers.WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
        "inputs = processor(audio, return_tensors=\"np\", sampling_rate=16000)\n",
        "features = ctranslate2.StorageView.from_array(inputs.input_features)\n",
        "\n",
        "# Describe the task in the prompt.\n",
        "# See the prompt format in https://github.com/openai/whisper.\n",
        "prompt = processor.tokenizer.convert_tokens_to_ids(\n",
        "    [\n",
        "        \"<|startoftranscript|>\",\n",
        "        \"<|ca|>\",\n",
        "        \"<|transcribe|>\",\n",
        "        \"<|notimestamps|>\",  # Remove this token to generate timestamps.\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "OQ6hTuVS3VKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the int8 model on CPU.\n",
        "model = ctranslate2.models.Whisper(\"whisper-tiny-ct2-int8\")\n",
        "\n",
        "# Detect the language.\n",
        "results = model.detect_language(features)\n",
        "language, probability = results[0][0]\n",
        "print(\"Detected language: %s with probability: %f\" % (language, probability))\n",
        "\n",
        "# Show the encoder results\n",
        "encoder_results = model.encode(features)\n",
        "print(\"Encoder results:\", encoder_results)\n",
        "\n",
        "# Run generation for the 30-second window.\n",
        "results = model.generate(features, [prompt], sampling_temperature=0, return_scores=True)\n",
        "transcription = processor.decode(results[0].sequences_ids[0])\n",
        "print(transcription)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "an2dOQOJ4lKD",
        "outputId": "c9d0256b-3b91-40e2-c4aa-b9c0cf081aec"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: <|ca|> with probability: 0.966133\n",
            "Encoder results:  0.102781 0.128416 0.00438075 ... 0.0382776 -0.605718 -0.177774\n",
            "[cpu:0 float32 storage viewed as 1x1500x384]\n",
            " 15 glaçons d'hydrogen. El podcast de l'engüa i tecnologia de sof català. Amb la és ideal. Vinga, vinga, no passa, no passa en que la taverna s'ompla. Sembla que no, però ja som a la pisòdinau.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial run\n",
        "# Detected language: <|ca|> with probability: 0.974995\n",
        "# Encoder results:  0.0998689 0.12546 0.0150658 ... 0.0410112 -0.61152 -0.173796\n",
        "# [cpu:0 float32 storage viewed as 1x1500x384]\n",
        "#  15 glaçons d'hydrogen. El podcast de l'engüa i tecnologia de sof català. Amb la és ideal. Vinga, vinga, no passa, no passa en que la taverna s'ompla. Sembla que no, però ja som a la pisòdinau.\n",
        "\n",
        "# Disconnect and rerun\n",
        "# Detected language: <|ca|> with probability: 0.966133\n",
        "# Encoder results:  0.102781 0.128416 0.00438075 ... 0.0382776 -0.605718 -0.177774\n",
        "# [cpu:0 float32 storage viewed as 1x1500x384]\n",
        "#  15 glaçons d'hydrogen. El podcast de l'engüa i tecnologia de sof català. Amb la és ideal. Vinga, vinga, no passa, no passa en que la taverna s'ompla. Sembla que no, però ja som a la pisòdinau."
      ],
      "metadata": {
        "id": "YBhuYK3oaBsC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the int8 model on GPU.\n",
        "model = ctranslate2.models.Whisper(\"whisper-tiny-ct2-int8\", device=\"cuda\")\n",
        "\n",
        "# Detect the language.\n",
        "results = model.detect_language(features)\n",
        "language, probability = results[0][0]\n",
        "print(\"Detected language: %s with probability: %f\" % (language, probability))\n",
        "\n",
        "# Show the encoder results\n",
        "encoder_results = model.encode(features)\n",
        "print(\"Encoder results:\", encoder_results)\n",
        "\n",
        "# Run generation for the 30-second window.\n",
        "results = model.generate(features, [prompt], sampling_temperature=0, return_scores=True)\n",
        "transcription = processor.decode(results[0].sequences_ids[0])\n",
        "print(transcription)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbVYDcE539Uq",
        "outputId": "8a7171db-7399-44a6-cec7-506beb190783"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: <|ca|> with probability: 0.974822\n",
            "Encoder results:  0.0937386 0.123954 0.0130554 ... 0.0373841 -0.595213 -0.162236\n",
            "[cuda:0 float32 storage viewed as 1x1500x384]\n",
            " 15 glaçons d'edragem. El podcast de l'engua i tecnologia de sof català. Amb la és ideal. Vinga, vinga, aneu passant, aneu passant que la taverna s'ompla. Sembla que no, però ja som a la pisòdinau.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial run\n",
        "# Detected language: <|ca|> with probability: 0.978478\n",
        "# Encoder results:  0.094764 0.123045 0.0108672 ... 0.0404294 -0.597082 -0.161252\n",
        "# [cuda:0 float32 storage viewed as 1x1500x384]\n",
        "#  15 glaçons d'edragem. El podcast de l'engua i tecnologia de sof català. Amb la és ideal. Vinga, vinga, aneu passant, aneu passant que la taverna s'ompla. Sembla que no, però ja som a la pisoni nou.\n",
        "\n",
        "# Disconnect and rerun\n",
        "# Detected language: <|ca|> with probability: 0.974822\n",
        "# Encoder results:  0.0937386 0.123954 0.0130554 ... 0.0373841 -0.595213 -0.162236\n",
        "# [cuda:0 float32 storage viewed as 1x1500x384]\n",
        "#  15 glaçons d'edragem. El podcast de l'engua i tecnologia de sof català. Amb la és ideal. Vinga, vinga, aneu passant, aneu passant que la taverna s'ompla. Sembla que no, però ja som a la pisòdinau."
      ],
      "metadata": {
        "id": "whgDniVBaCja"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fp32 model on CPU.\n",
        "model = ctranslate2.models.Whisper(\"whisper-tiny-ct2\")\n",
        "\n",
        "# Detect the language.\n",
        "results = model.detect_language(features)\n",
        "language, probability = results[0][0]\n",
        "print(\"Detected language: %s with probability: %f\" % (language, probability))\n",
        "\n",
        "# Show the encoder results\n",
        "encoder_results = model.encode(features)\n",
        "print(\"Encoder results:\", encoder_results)\n",
        "\n",
        "# Run generation for the 30-second window.\n",
        "results = model.generate(features, [prompt], sampling_temperature=0, return_scores=True)\n",
        "transcription = processor.decode(results[0].sequences_ids[0])\n",
        "print(transcription)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxaHB--x5v-w",
        "outputId": "7b149105-b998-4f79-b5ea-78dd3d66918b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: <|ca|> with probability: 0.974038\n",
            "Encoder results:  0.12497 0.117955 0.0298516 ... 0.0300566 -0.589291 -0.168419\n",
            "[cpu:0 float32 storage viewed as 1x1500x384]\n",
            " 15 glaçons d'edragem. El podcast de l'engua i tecnologia de sof català. Amb la és ideal. Vinga, vinga, no ha passat, no ha passat que la taverna s'ompla. Sembla que no, però ja som a la pisòdinau.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial run\n",
        "# Detected language: <|ca|> with probability: 0.974038\n",
        "# Encoder results:  0.12497 0.117955 0.0298516 ... 0.0300566 -0.589291 -0.168419\n",
        "# [cpu:0 float32 storage viewed as 1x1500x384]\n",
        "#  15 glaçons d'edragem. El podcast de l'engua i tecnologia de sof català. Amb la és ideal. Vinga, vinga, no ha passat, no ha passat que la taverna s'ompla. Sembla que no, però ja som a la pisòdinau.\n",
        "\n",
        "# Disconnect and rerun\n",
        "# etected language: <|ca|> with probability: 0.974038\n",
        "# Encoder results:  0.12497 0.117955 0.0298516 ... 0.0300567 -0.589291 -0.168419\n",
        "# [cpu:0 float32 storage viewed as 1x1500x384]\n",
        "#  15 glaçons d'edragem. El podcast de l'engua i tecnologia de sof català. Amb la és ideal. Vinga, vinga, no ha passat, no ha passat que la taverna s'ompla. Sembla que no, però ja som a la pisòdinau."
      ],
      "metadata": {
        "id": "Zm-ZtJa9aDc-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fp32 model on GPU.\n",
        "model = ctranslate2.models.Whisper(\"whisper-tiny-ct2\", device=\"cuda\")\n",
        "\n",
        "# Detect the language.\n",
        "results = model.detect_language(features)\n",
        "language, probability = results[0][0]\n",
        "print(\"Detected language: %s with probability: %f\" % (language, probability))\n",
        "\n",
        "# Show the encoder results\n",
        "encoder_results = model.encode(features)\n",
        "print(\"Encoder results:\", encoder_results)\n",
        "\n",
        "# Run generation for the 30-second window.\n",
        "results = model.generate(features, [prompt], sampling_temperature=0, return_scores=True)\n",
        "transcription = processor.decode(results[0].sequences_ids[0])\n",
        "print(transcription)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gpnBCy45wsQ",
        "outputId": "f89a7aa9-929a-4341-f221-55a936a373c4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: <|ca|> with probability: 0.974039\n",
            "Encoder results:  0.124969 0.117955 0.029852 ... 0.0300568 -0.589291 -0.168419\n",
            "[cuda:0 float32 storage viewed as 1x1500x384]\n",
            " 15 glaçons d'edragem. El podcast de l'engua i tecnologia de sof català. Amb la és ideal. Vinga, vinga, no ha passat, no ha passat que la taverna s'ompla. Sembla que no, però ja som a la pisòdinau.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial run\n",
        "# Detected language: <|ca|> with probability: 0.974039\n",
        "# Encoder results:  0.124969 0.117955 0.0298519 ... 0.0300569 -0.589291 -0.168419\n",
        "# [cuda:0 float32 storage viewed as 1x1500x384]\n",
        "#  15 glaçons d'edragem. El podcast de l'engua i tecnologia de sof català. Amb la és ideal. Vinga, vinga, no ha passat, no ha passat que la taverna s'ompla. Sembla que no, però ja som a la pisòdinau.\n",
        "\n",
        "# Disconnect and rerun\n",
        "# Detected language: <|ca|> with probability: 0.974039\n",
        "# Encoder results:  0.124969 0.117955 0.029852 ... 0.0300568 -0.589291 -0.168419\n",
        "# [cuda:0 float32 storage viewed as 1x1500x384]\n",
        "#  15 glaçons d'edragem. El podcast de l'engua i tecnologia de sof català. Amb la és ideal. Vinga, vinga, no ha passat, no ha passat que la taverna s'ompla. Sembla que no, però ja som a la pisòdinau."
      ],
      "metadata": {
        "id": "neIO19QPaPwq"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}